{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# einops: phi & qwen\n",
    "# sentencepiece: llama\n",
    "# tiktoken & transformers_stream_generator: qwen\n",
    "!pip install transformers trl wandb peft einops sentencepiece tiktoken transformers_stream_generator==0.0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# when using quantized models\n",
    "\n",
    "!pip install auto-gptq optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only on first run.\n",
    "\n",
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "\n",
    "torch.distributed.init_process_group(backend='nccl', world_size=1, rank=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "            model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            reward_model=\"text-classification:Hello-SimpleAI/chatgpt-detector-roberta\",\n",
    "            learning_rate=1.41e-5,\n",
    "            log_with=\"wandb\", \n",
    "            mini_batch_size=8, # really important for memory\n",
    "            batch_size=128, \n",
    "            gradient_accumulation_steps=1,\n",
    "            early_stopping=False,\n",
    "            target_kl=6.0,\n",
    "            kl_penalty=\"kl\",\n",
    "            seed=0,\n",
    "            use_score_scaling=False,\n",
    "            use_score_norm=False,\n",
    "            score_clip=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name,\n",
    "                                          padding_side='left',\n",
    "                                          trust_remote_code=True) \n",
    "\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import build_dataset\n",
    "\n",
    "dataset = build_dataset(tokenizer)\n",
    "    \n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = dataset[0]['input_ids'].to('cuda')\n",
    "test1 = model.generate(inputs, **generation_kwargs)  \n",
    "test2 = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(test1, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant who gives detailed and long answers\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Explain Tuckman's 4 stages of development.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_tensors='pt')\n",
    "test = model.generate(input_ids.to('cuda'), **generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_ids_padded = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, max_length=300, padding='max_length', return_dict=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the attention mask to the model is necessary for correct generation when padding!\n",
    "\n",
    "test_padded = model.generate(input_ids_padded['input_ids'].to('cuda'), \n",
    "                             attention_mask=input_ids_padded['attention_mask'].to('cuda'),\n",
    "                             **generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_device = Accelerator().local_process_index\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\",) \n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\",) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer\n",
    "The `PPOTrainer` takes care of device placement and optimization later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from trl.import_utils import is_xpu_available\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "task, model_name = config.reward_model.split(\":\")\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "\n",
    "task, model_name = config.reward_model.split(\":\")\n",
    "classifier_pipe = pipeline(task, model=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "if classifier_pipe.tokenizer.pad_token_id is None:\n",
    "    classifier_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if classifier_pipe.model.config.pad_token_id is None:\n",
    "    classifier_pipe.model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "test = classifier_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0][0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation settings\n",
    "For the response generation we just use sampling and make sure top-k and nucleus sampling are turned off as well as a minimal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\n",
    "    \"truncation\": True, \n",
    "    \"max_length\": 512, # base model context length\n",
    "    \"return_all_scores\": True, \n",
    "    \"function_to_apply\": \"none\", \n",
    "    \"batch_size\": config.mini_batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 356,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ppo_trainer.dataloader))\n",
    "# query_tensors = batch[\"input_ids\"]\n",
    "query_tensors = torch.LongTensor([q.tolist() for q in batch['input_ids']]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tensors = model.generate(\n",
    "    query_tensors,\n",
    "    return_dict_in_generate=False,\n",
    "    generation_config=model.generation_config\n",
    ")\n",
    "ref_response_tensors = ref_model.generate(\n",
    "    query_tensors,\n",
    "    return_dict_in_generate=False,\n",
    "    generation_config=ref_model.generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_repsonse_from_qwen_batch\n",
    "\n",
    "batch[\"response\"] = get_repsonse_from_qwen_batch(tokenizer, response_tensors, query_tensors, batch['query'])\n",
    "batch[\"ref_response\"] = get_repsonse_from_qwen_batch(tokenizer, response_tensors, query_tensors, batch['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get response from model\n",
    "response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
    "    query_tensors, return_prompt=False, generate_ref_response=True, **model.generation_config.to_dict()\n",
    ")\n",
    "batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentiment score\n",
    "# texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "# pipe_outputs = classifier_pipe(texts, **sent_kwargs)\n",
    "pipe_outputs = classifier_pipe(batch['response'], **sent_kwargs)\n",
    "rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
    "# ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n",
    "# ref_pipe_outputs = classifier_pipe(ref_texts, **sent_kwargs)\n",
    "ref_pipe_outputs = classifier_pipe(batch['ref_response'], **sent_kwargs)\n",
    "ref_rewards = [torch.tensor(output[0][\"score\"]) for output in ref_pipe_outputs]\n",
    "batch[\"ref_rewards\"] = ref_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for r in rewards:\n",
    "    total += r\n",
    "print(total/len(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tensors_list = [rt for rt in response_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run PPO step\n",
    "stats = ppo_trainer.step(batch[\"input_ids\"], response_tensors_list, rewards) # use batch[\"input_ids\"] instead of query_tensors for weights upate because you need a list here\n",
    "ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop consists of the following main steps:\n",
    "1. Get the query responses from the policy network (GPT-2)\n",
    "2. Get sentiments for query/responses from BERT\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet\n",
    "\n",
    "**Training time**\n",
    "\n",
    "This step takes **~2h** on a V100 GPU with the above specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from gpt2\n",
    "    response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)\n",
    "\n",
    "    # Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = classifier_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
    "    ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n",
    "    ref_pipe_outputs = classifier_pipe(ref_texts, **sent_kwargs)\n",
    "    ref_rewards = [torch.tensor(output[0][\"score\"]) for output in ref_pipe_outputs]\n",
    "    batch[\"ref_rewards\"] = ref_rewards\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training progress\n",
    "If you are tracking the training progress with Weights&Biases you should see a plot similar to the one below. Check out the interactive sample report on wandb.ai: [link](https://app.wandb.ai/huggingface/trl-showcase/runs/1jtvxb1m/).\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_tuning_progress.png' width='800'>\n",
    "<p style=\"text-align: center;\"> <b>Figure:</b> Reward mean and distribution evolution during training. </p>\n",
    "</div>\n",
    "\n",
    "One can observe how the model starts to generate more positive outputs after a few optimisation steps.\n",
    "\n",
    "> Note: Investigating the KL-divergence will probably show that at this point the model has not converged to the target KL-divergence, yet. To get there would require longer training or starting with a higher initial coefficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inspection\n",
    "Let's inspect some examples from the IMDB dataset. We can use `model_ref` to compare the tuned model `model` against the model before optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences we observe a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "Finally, we save the model and push it to the Hugging Face for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)\n",
    "tokenizer.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
